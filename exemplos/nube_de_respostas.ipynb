{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nube_de_respostas.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOiatDXQqEOthlfcaxzUXa1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bigdatawirtz/sistemas-bigdata/blob/main/exemplos/nube_de_respostas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNgB-24L4DoV"
      },
      "source": [
        "**Créditos**: baseado nun código visto en: https://medium.com/@a.fernandez.troyano/nube-de-palabras-word-cloud-con-python-a-partir-de-varias-webs-111e94220822"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYHUlxpe3C8N"
      },
      "source": [
        "**Librarías**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV5xxIb_xPjM"
      },
      "source": [
        "#Librerías básicas utiliadas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Librerías necesarias para abrir imágenes, generar nube de palabras y plot\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Librerías necesarias para la limpieza de datos\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBDz75RT0_NX"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8gnQoRJ3Kz9"
      },
      "source": [
        "**Carga de texto**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO-EWcqtyanY"
      },
      "source": [
        "texto = \"\"\"\n",
        "El Big Data es una cosa muy buena. Muy buena tiene que ser, y muy grande, por lo de Big.\n",
        "El Big Data es cuando tratas muchos datos.\n",
        "El Big Data es utilizar muchos datos para extraer información.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLm5R967zHs2"
      },
      "source": [
        "texto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn-LBb_z3X_5"
      },
      "source": [
        "**Limpeza do texto**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVDLW9HD0kcX"
      },
      "source": [
        "#Generación de lista de signos de puntuación\n",
        "\n",
        "punctuation=[]\n",
        "for s in string.punctuation:\n",
        "    punctuation.append(str(s))\n",
        "sp_punctuation = [\"¿\", \"¡\", \"“\", \"”\", \"…\", \":\", \"–\", \"»\", \"«\"]    \n",
        "\n",
        "punctuation += sp_punctuation\n",
        "\n",
        "punctuation[:10] #Ejemplo de los símbolos de puntuación que están incluidos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDYp8z770sxj"
      },
      "source": [
        "#Listado de palabras que queremos eliminar del texto\n",
        "#Es un proceso iterativo por lo que si después vemos que nos siguen quedado \"caractéres raros\" simplemente venímos aquí y los agregamos\n",
        "#Existe librerías y listados de \"Stop_words\", pero por ahora vamos a dejarlo vacío\n",
        "\n",
        "#nltk.download('stopwords') #La primera vez debemos descargar las \"stopwords\"\n",
        "\n",
        "stop_words = stopwords.words('spanish') #Listado de palabras a eliminar\n",
        "\n",
        "stop_words += [\"\\u200b\", \"\\xa0\", \"para\", \"como\", \"puede\",\"cómo\", \"hacer\", \"forma\", \"parte\", \"hace\", \"además\", \"según\", \"pueden\", \"ser\"] #Añadimos algunos caractéres que hemos encontrado\n",
        "\n",
        "stop_words[:10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2W7n4WF1Jew"
      },
      "source": [
        "#Reemplazamos signos de puntuación por \"\":\n",
        "for p in punctuation:\n",
        "    clean_texto = texto.lower().replace(p,\"\")\n",
        "    \n",
        "for p in punctuation:\n",
        "    clean_texto = clean_texto.replace(p,\"\")\n",
        "\n",
        "#Eliminamos espacios blancos, saltos de línea, tabuladores, etc    \n",
        "#clean_texto = \" \".join(clean_texto.split())    \n",
        "\n",
        "#Reemplazamos stop_words por \"\":    \n",
        "for stop in stop_words:\n",
        "    clean_texto_list = clean_texto.split()\n",
        "    clean_texto_list = [i.strip() for i in clean_texto_list]\n",
        "    try:\n",
        "        while stop in clean_texto_list: clean_texto_list.remove(stop)\n",
        "    except:\n",
        "        print(\"Error\")\n",
        "        pass\n",
        "    clean_texto= \" \".join(clean_texto_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UBUs-Mz0wTm"
      },
      "source": [
        "lista_texto = clean_texto.split(\" \")\n",
        "\n",
        "palabras = []\n",
        "\n",
        "#Paso intermedio para eliminar palabras muy cortas (emoticonos,etc) y muy largas (ulr o similar) que se nos hayan pasado:\n",
        "\n",
        "for palabra in lista_texto:\n",
        "    if (len(palabra)>=3 and len(palabra)<18):\n",
        "        palabras.append(palabra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCFWApD23pt_"
      },
      "source": [
        "**Contaxe de palabras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IAeRVsp1R0K"
      },
      "source": [
        "#Generamos un diccionario para contabilizar las palabras:\n",
        "\n",
        "word_count={}\n",
        "\n",
        "for palabra in palabras:\n",
        "    if palabra in word_count.keys():\n",
        "        word_count[palabra][0]+=1\n",
        "    else:\n",
        "        word_count[palabra]=[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYE-U4m51T1M"
      },
      "source": [
        "#Generamos el DF y lo ordenamos:\n",
        "\n",
        "df = pd.DataFrame.from_dict(word_count).transpose()\n",
        "df.columns=[\"freq\"]\n",
        "df.sort_values([\"freq\"], ascending=False, inplace=True)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-1YOQex3wdU"
      },
      "source": [
        "**Mostrar gráfica de ocorrencias das palabras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IZc7CUk1V8n"
      },
      "source": [
        "def plot_bar(data=df, top=5):    \n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([0,0,2,1])\n",
        "    ax.bar(x =df.iloc[:top,:].index, height = df.iloc[:top,0].values)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykL_y-oJ32Nx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n_tViiV1Y1O"
      },
      "source": [
        "#Graficamos el TOP 5 palabras por frecuencia\n",
        "\n",
        "plot_bar(data=df, top=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DipDVZS341f"
      },
      "source": [
        "**Crear nube de palabras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2E38xVL1bUP"
      },
      "source": [
        "#word_cloud = WordCloud(height=800, width=800, background_color='white',max_words=150, min_font_size=5, collocation_threshold=10).generate(clean_texto)\n",
        "word_cloud = WordCloud(height=800, width=800, background_color='white',max_words=150, min_font_size=5).generate(clean_texto)\n",
        "\n",
        "#word_cloud.to_file(\"./img/ejemplo_sencillo.png\") #Guardamos la imagen generada\n",
        "#word_cloud.to_file(\"./img/ejemplo_sencillo.png\") #Guardamos la imagen generada\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(word_cloud)\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izhn-N031e3n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}